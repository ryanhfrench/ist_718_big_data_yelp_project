{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"yelp_logo.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark import sql\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset available at: https://www.kaggle.com/yelp-dataset/yelp-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6685900"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/ryanfrench/Dropbox/cloud_documents/college/9_fall_2019/IST_718/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "reviews_df = spark.read.json(path)\n",
    "reviews_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER TO REMOVE THIS, THIS SAMPLES ONLY 1/1000 OF THE DATA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.sample(False, 0.001, seed = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- cool: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- funny: long (nullable = true)\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- stars: double (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- useful: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema\n",
    "reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of NULL values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_id : 0\n",
      "cool : 0\n",
      "date : 0\n",
      "funny : 0\n",
      "review_id : 0\n",
      "stars : 0\n",
      "text : 0\n",
      "useful : 0\n",
      "user_id : 0\n"
     ]
    }
   ],
   "source": [
    "for col in reviews_df.columns:\n",
    "    print(col, ':', reviews_df.filter(reviews_df[col].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|         business_id|cool|               date|funny|           review_id|stars|                text|useful|             user_id|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "|hcFSc0OHgZJybnjQB...|   0|2014-02-05 18:10:23|    0|4CMCRoNZgmxwtOBLG...|  5.0|Fresher noodles y...|     0|5BfyObsvVjB6zcj8W...|\n",
      "|HhVmDybpU7L50Kb5A...|   0|2017-06-06 19:51:49|    0|QdXaCGCQtvSiue-0m...|  2.0|I would not be go...|     0|zPByj_3y6TBok5BpC...|\n",
      "|XXCIO_TXCE1uQWPLr...|   1|2015-03-04 19:51:08|    1|uqAUjIy1OKnNl1rzJ...|  1.0|Don't waste your ...|     1|EZxdmp1_mXzPaHreZ...|\n",
      "|Hpi9raHlFm6prTSzF...|   1|2018-07-07 21:56:54|    1|UldTdufMzctB0llHL...|  2.0|On a second visit...|     2|moj7DUHkP4j9yfwFm...|\n",
      "|cisqOxAgQPjVVr9Yz...|   0|2018-07-26 14:25:15|    0|ylZjXOIZZqZPBRztX...|  4.0|I absolutely love...|     0|TX5kwQLpKUDvNf4mf...|\n",
      "+--------------------+----+-------------------+-----+--------------------+-----+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6585"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.show(5)\n",
    "reviews_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we predict high star Yelp reviews based off of their text content?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By creating a text transformation and prediction pipeline can we predict which Yelp reviews will have a high number of stars (4/5 or above)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dummy variable for high reviews (defined as a 4/5 or above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By creating a dummy variable we can easily predict the binary outcomes of low vs high reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|d_stars|count|\n",
      "+-------+-----+\n",
      "|      1| 4341|\n",
      "|      0| 2244|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df = reviews_df.withColumn('d_stars',when(col('stars') >= 4, 1).otherwise(0))\n",
    "reviews_df = reviews_df.select('d_stars', 'text')\n",
    "reviews_df.groupBy('d_stars').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove special characters from reviews and create new column containing filtered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "reviews_df = reviews_df.withColumn('f_text', regexp_replace(col('text'), '[.,/#!$%^&*;:{}=_`~()-]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|f_text                                                                                                                                                                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Fresher noodles you may not find in this town I tried the steamed vegetable dumplings and the chicken noodle soup  loved it But careful the portions are big but the staff box it nicely and it still that's great next day \n",
      "Also the mango bubble tea was tasty|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.select('f_text').show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "reviews_df = reviews_df.withColumn('f_text', regexp_replace(col('f_text'), '[\\d-]', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|f_text                                                                                                                                                                                                                                                          |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Fresher noodles you may not find in this town I tried the steamed vegetable dumplings and the chicken noodle soup  loved it But careful the portions are big but the staff box it nicely and it still that's great next day \n",
      "Also the mango bubble tea was tasty|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.select('f_text').show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce instances of multiple whitespaces to just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.withColumn('f_text', regexp_replace(col('f_text'),\"\\\\s+\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|f_text                                                                                                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Fresher noodles you may not find in this town I tried the steamed vegetable dumplings and the chicken noodle soup loved it But careful the portions are big but the staff box it nicely and it still that's great next day Also the mango bubble tea was tasty|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.select('f_text').show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all words to lowercase for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "reviews_df = reviews_df.withColumn('f_text', lower(col('f_text')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|f_text                                                                                                                                                                                                                                                        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|fresher noodles you may not find in this town i tried the steamed vegetable dumplings and the chicken noodle soup loved it but careful the portions are big but the staff box it nicely and it still that's great next day also the mango bubble tea was tasty|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_df.select('f_text').show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize 'f_text' column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By tokenizing the 'f_text' column we can break out the long strings of our formatted review text into their individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer().setInputCol('f_text').setOutputCol('tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample output of Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+\n",
      "|d_stars|                text|              f_text|              tokens|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|      1|Fresher noodles y...|fresher noodles y...|[fresher, noodles...|\n",
      "|      0|I would not be go...|i would not be go...|[i, would, not, b...|\n",
      "|      0|Don't waste your ...|don't waste your ...|[don't, waste, yo...|\n",
      "|      0|On a second visit...|on a second visit...|[on, a, second, v...|\n",
      "|      1|I absolutely love...|i absolutely love...|[i, absolutely, l...|\n",
      "|      0|This place USED t...|this place used t...|[this, place, use...|\n",
      "|      0|Beware. My dog wa...|beware my dog was...|[beware, my, dog,...|\n",
      "|      1|First time here w...|first time here w...|[first, time, her...|\n",
      "|      1|Plumbing Update w...|plumbing update w...|[plumbing, update...|\n",
      "|      1|I frequent this p...|i frequent this p...|[i, frequent, thi...|\n",
      "|      1|This is my second...|this is my second...|[this, is, my, se...|\n",
      "|      0|The front desk st...|the front desk st...|[the, front, desk...|\n",
      "|      1|Great service, sm...|great service smi...|[great, service, ...|\n",
      "|      1|Love this place! ...|love this place i...|[love, this, plac...|\n",
      "|      1|This is so much f...|this is so much f...|[this, is, so, mu...|\n",
      "|      1|The pollo mole is...|the pollo mole is...|[the, pollo, mole...|\n",
      "|      1|We recently hit u...|we recently hit u...|[we, recently, hi...|\n",
      "|      1|I really liked th...|i really liked th...|[i, really, liked...|\n",
      "|      0|This just the caf...|this just the caf...|[this, just, the,...|\n",
      "|      0|Scrambled eggs ar...|scrambled eggs ar...|[scrambled, eggs,...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.transform(reviews_df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NTLK stop words from GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop words are words which are generally considered to offer little insight in linguistic processing due to the fact that they are utilized so frequently. We will be utilizing the list contained in the Natural Language Tool Kit (NLTK) to remove these words in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    stop_words = requests.get('https://gist.github.com/sebleier/554280').text.split()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sw_filter = StopWordsRemover(). \\\n",
    "            setStopWords(stop_words). \\\n",
    "            setCaseSensitive(False). \\\n",
    "            setInputCol(\"tokens\"). \\\n",
    "            setOutputCol(\"filtered_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADJUST THE MINDF PARAM WITH FULL REVIEWS TAKEN INTO ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CountVectorizer Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CountVectorizer counts the number of times that a token appears in each review and then saves this list of numbers as a vector for each review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# we will remove words that appear in 50 docs or less\n",
    "cv = CountVectorizer(minTF=1., minDF=6, vocabSize=2**17). \\\n",
    "     setInputCol(\"filtered_tokens\"). \\\n",
    "     setOutputCol(\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create initial text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cv_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv]).fit(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample output of initial text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|d_stars|                text|              f_text|              tokens|     filtered_tokens|                  tf|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      1|Fresher noodles y...|fresher noodles y...|[fresher, noodles...|[fresher, noodles...|(4978,[3,9,13,22,...|\n",
      "|      0|I would not be go...|i would not be go...|[i, would, not, b...|[back, personally...|(4978,[1,2,6,11,1...|\n",
      "|      0|Don't waste your ...|don't waste your ...|[don't, waste, yo...|[waste, money, mi...|(4978,[4,22,31,35...|\n",
      "|      0|On a second visit...|on a second visit...|[on, a, second, v...|[visit, changed, ...|(4978,[5,6,19,59,...|\n",
      "|      1|I absolutely love...|i absolutely love...|[i, absolutely, l...|[absolutely, chat...|(4978,[0,3,5,6,8,...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_pipeline.transform(reviews_df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inverse Document Frequency (IDF) stage for the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will lower the realtive value of tokens which appear frequently in documents allowing those that are more specialized to stand out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "idf = IDF(). \\\n",
    "      setInputCol('tf'). \\\n",
    "      setOutputCol('tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create IDF text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_pipeline = Pipeline(stages=[cv_pipeline, idf]).fit(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample output of IDF text processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|d_stars|                text|              f_text|              tokens|     filtered_tokens|                  tf|               tfidf|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|      1|Fresher noodles y...|fresher noodles y...|[fresher, noodles...|[fresher, noodles...|(4978,[3,9,13,22,...|(4978,[3,9,13,22,...|\n",
      "|      0|I would not be go...|i would not be go...|[i, would, not, b...|[back, personally...|(4978,[1,2,6,11,1...|(4978,[1,2,6,11,1...|\n",
      "|      0|Don't waste your ...|don't waste your ...|[don't, waste, yo...|[waste, money, mi...|(4978,[4,22,31,35...|(4978,[4,22,31,35...|\n",
      "|      0|On a second visit...|on a second visit...|[on, a, second, v...|[visit, changed, ...|(4978,[5,6,19,59,...|(4978,[5,6,19,59,...|\n",
      "|      1|I absolutely love...|i absolutely love...|[i, absolutely, l...|[absolutely, chat...|(4978,[0,3,5,6,8,...|(4978,[0,3,5,6,8,...|\n",
      "+-------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf_pipeline.transform(reviews_df).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Text Frequency Inverse Document Frequency (TFIDF) Reviews Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By passing the reviews data frame to the IDF pipeline we can create a TFIDF dataframe which we will then use to predict the review scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = idf_pipeline.transform(reviews_df)\n",
    "pd_tfidf_df = tfidf_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d_stars', 'text', 'f_text', 'tokens', 'filtered_tokens', 'tf', 'tfidf']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d_stars</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>(0.0, 0.0, 0.0, 1.1646703360059747, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 1.1171554603984615, 1.1143751064294236, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 1.2003353344165135, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>(0.0, 0.0, 0.0, 0.0, 0.0, 2.7610822559822092, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>(1.1208746650575285, 0.0, 0.0, 1.1646703360059...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   d_stars                                              tfidf\n",
       "0        1  (0.0, 0.0, 0.0, 1.1646703360059747, 0.0, 0.0, ...\n",
       "1        0  (0.0, 1.1171554603984615, 1.1143751064294236, ...\n",
       "2        0  (0.0, 0.0, 0.0, 0.0, 1.2003353344165135, 0.0, ...\n",
       "3        0  (0.0, 0.0, 0.0, 0.0, 0.0, 2.7610822559822092, ...\n",
       "4        1  (1.1208746650575285, 0.0, 0.0, 1.1646703360059..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_df = pd_tfidf_df[['d_stars', 'tfidf']]\n",
    "lr_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prediction pipeline for estimating high star reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training, validation, and testing groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = reviews_df.randomSplit([0.6, 0.3, 0.1], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of training records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3942"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of validation records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of testing records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build logistic regression model for later predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lr = LogisticRegression(). \\\n",
    "     setLabelCol('d_stars'). \\\n",
    "     setFeaturesCol('tfidf'). \\\n",
    "     setRegParam(0.0). \\\n",
    "     setMaxIter(100). \\\n",
    "     setElasticNetParam(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new pipeline for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lr_estimator = Pipeline(stages=[tokenizer, \n",
    "                                   sw_filter, \n",
    "                                   cv, \n",
    "                                   idf, \n",
    "                                   en_lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build grid search estimator pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This will allow us to iteratively search for the optimal $\\lambda$ and $\\alpha$ parameters to achieve the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_ad20fe4bf303,\n",
       " StopWordsRemover_328a0da46bfc,\n",
       " CountVectorizer_296e570599a4,\n",
       " IDF_9ca5c62c34dd,\n",
       " LogisticRegression_c71a2c6508af]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_lr_estimator.getStages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a pipeline transformation by chaining the `idf_pipeline` with the logistic regression step (`lr`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder(). \\\n",
    "       addGrid(en_lr.regParam, [0., 0.01, 0.02]). \\\n",
    "       addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]). \\\n",
    "       build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View grid parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.02,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.02,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2},\n",
       " {Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.02,\n",
       "  Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.4}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1\n",
      "Fitting model 2\n",
      "Fitting model 3\n",
      "Fitting model 4\n",
      "Fitting model 5\n",
      "Fitting model 6\n",
      "Fitting model 7\n",
      "Fitting model 8\n",
      "Fitting model 9\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "\n",
    "for j in range(len(grid)):\n",
    "    print(\"Fitting model {}\".format(j+1))\n",
    "    model = en_lr_estimator.fit(train_df, grid[j])\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "accuracies = [m. \\\n",
    "              transform(validation_df). \\\n",
    "              select(fn.avg(fn.expr('float(d_stars = prediction)')).alias('accuracy')). \\\n",
    "              first(). \\\n",
    "              accuracy for m in all_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7967278135845315,\n",
       " 0.7967278135845315,\n",
       " 0.7967278135845315,\n",
       " 0.819533961328706,\n",
       " 0.8349033217649975,\n",
       " 0.8393653941497273,\n",
       " 0.8215171046108082,\n",
       " 0.8413485374318295,\n",
       " 0.8304412493802678]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model index = 7\n"
     ]
    }
   ],
   "source": [
    "best_model_idx = np.argmax(accuracies)\n",
    "print(\"best model index =\", best_model_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the grid parameters of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_c71a2c6508af', name='regParam', doc='regularization parameter (>= 0).'): 0.02,\n",
       " Param(parent='LogisticRegression_c71a2c6508af', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.2}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = all_models[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8413485374318295"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies[best_model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          accuracy|\n",
      "+------------------+\n",
      "|0.8370607028753994|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimate generalization performance\n",
    "best_model. \\\n",
    "    transform(test_df). \\\n",
    "    select(fn.avg(fn.expr('float(d_stars = prediction)')).alias('accuracy')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a pipeline transformation by chaining the `idf_pipeline` with the logistic regression step (`lr`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lr_pipeline = Pipeline(stages=[idf_pipeline, en_lr]).fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets estimate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(correct)|\n",
      "+------------------+\n",
      "|0.8006941001487358|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_lr_pipeline.transform(validation_df). \\\n",
    "            select(fn.expr('float(prediction = d_stars)').alias('correct')). \\\n",
    "            select(fn.avg('correct')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most significant words in the prediction of both high and low reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_weights = en_lr_pipeline.stages[-1].coefficients.toArray()\n",
    "en_coeffs_df = pd.DataFrame({'word': en_lr_pipeline.stages[0].stages[0].stages[2].vocabulary,\n",
    "                             'weight': en_weights})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View most significant words indicating high reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>turning</td>\n",
       "      <td>19.150099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4910</th>\n",
       "      <td>difficulty</td>\n",
       "      <td>14.536793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3860</th>\n",
       "      <td>o'clock</td>\n",
       "      <td>14.088663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>aussi</td>\n",
       "      <td>12.754782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great</td>\n",
       "      <td>12.253716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>grew</td>\n",
       "      <td>11.430257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>belle</td>\n",
       "      <td>10.819681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>amazing</td>\n",
       "      <td>10.659649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3246</th>\n",
       "      <td>café</td>\n",
       "      <td>10.490388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>maintained</td>\n",
       "      <td>10.269260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>delicious</td>\n",
       "      <td>10.226091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>powder</td>\n",
       "      <td>10.096770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4876</th>\n",
       "      <td>tolerance</td>\n",
       "      <td>9.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>triple</td>\n",
       "      <td>9.760199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4717</th>\n",
       "      <td>comparing</td>\n",
       "      <td>9.732649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     weight\n",
       "4085     turning  19.150099\n",
       "4910  difficulty  14.536793\n",
       "3860     o'clock  14.088663\n",
       "4153       aussi  12.754782\n",
       "3          great  12.253716\n",
       "1957        grew  11.430257\n",
       "4141       belle  10.819681\n",
       "17       amazing  10.659649\n",
       "3246        café  10.490388\n",
       "2516  maintained  10.269260\n",
       "20     delicious  10.226091\n",
       "4643      powder  10.096770\n",
       "4876   tolerance   9.859800\n",
       "2933      triple   9.760199\n",
       "4717   comparing   9.732649"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_coeffs_df.sort_values('weight', ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View most significant words indicating low reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4763</th>\n",
       "      <td>cobb</td>\n",
       "      <td>-15.558516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>squeeze</td>\n",
       "      <td>-14.420943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>passable</td>\n",
       "      <td>-14.415062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137</th>\n",
       "      <td>freezing</td>\n",
       "      <td>-14.284611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4682</th>\n",
       "      <td>rocking</td>\n",
       "      <td>-14.231040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4415</th>\n",
       "      <td>sur</td>\n",
       "      <td>-13.191218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>overdone</td>\n",
       "      <td>-12.819695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4547</th>\n",
       "      <td>plays</td>\n",
       "      <td>-12.535362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>bally's</td>\n",
       "      <td>-12.053363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4826</th>\n",
       "      <td>tidy</td>\n",
       "      <td>-11.886709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4021</th>\n",
       "      <td>assistance</td>\n",
       "      <td>-11.714141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4889</th>\n",
       "      <td>headache</td>\n",
       "      <td>-11.629785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>dense</td>\n",
       "      <td>-11.572903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>ideal</td>\n",
       "      <td>-11.497297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>refilling</td>\n",
       "      <td>-11.279024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word     weight\n",
       "4763        cobb -15.558516\n",
       "4201     squeeze -14.420943\n",
       "4496    passable -14.415062\n",
       "4137    freezing -14.284611\n",
       "4682     rocking -14.231040\n",
       "4415         sur -13.191218\n",
       "3657    overdone -12.819695\n",
       "4547       plays -12.535362\n",
       "4257     bally's -12.053363\n",
       "4826        tidy -11.886709\n",
       "4021  assistance -11.714141\n",
       "4889    headache -11.629785\n",
       "2255       dense -11.572903\n",
       "2555       ideal -11.497297\n",
       "4054   refilling -11.279024"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_coeffs_df.sort_values('weight').head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
